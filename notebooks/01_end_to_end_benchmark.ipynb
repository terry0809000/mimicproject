{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# End-to-End MIMIC SDoH Benchmarking Notebook\n\nThis notebook implements a reproducible benchmarking workflow for extracting SDoH/SBDH labels from MIMIC-III discharge summaries using MIMIC-SBDH expert annotations."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## A. Setup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install dependencies (Colab only)\nimport sys\nimport subprocess\nfrom pathlib import Path\n\nif 'google.colab' in sys.modules:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'])\n\n# Standard imports\nimport json\nimport logging\nimport os\nimport random\nimport time\nimport importlib.util\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# Logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')\n\n# Environment info\nprint('Python:', sys.version)\nprint('Torch:', torch.__version__)\nprint('CUDA available:', torch.cuda.is_available())\n\n# Add repo root to sys.path\nrepo_root = Path.cwd()\nif (repo_root / 'src').exists():\n    sys.path.append(str(repo_root))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## B. Configuration"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass BenchmarkConfig:\n    mimic_root: str = r\"C:\\Users\\Terry Yu\\Documents\\mimic-iii-clinical-database-1.4\"\n    sbdh_path: str = r\"D:\\Social Determinants Research\\MIMIC DATASETS\\MIMIC-SBDH.csv\"\n    output_dir: str = \"outputs\"\n    category_filter: Optional[str] = \"Discharge summary\"  # set to None for all notes\n    task_type: str = \"multilabel\"  # 'multilabel' or 'binary'\n    label_columns: Optional[list[str]] = None  # auto-detect if None\n    chunksize: int = 50_000\n    max_length: int = 256\n    text_policy: str = \"truncate\"  # 'truncate' or 'sliding'\n    train_size: float = 0.7\n    val_size: float = 0.15\n    test_size: float = 0.15\n\nconfig = BenchmarkConfig()\nprint(config)\n\n# Optional: Google Drive mounting for Colab\n# from google.colab import drive\n# drive.mount('/content/drive')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## C. Data Loading & Dataset Construction"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pathlib import Path\nfrom src.data_access.real_loader import RealDatasetConfig, load_mimic_sbdh_dataset\n\noutput_dir = Path(config.output_dir)\n(output_dir / 'splits').mkdir(parents=True, exist_ok=True)\n(output_dir / 'metrics').mkdir(parents=True, exist_ok=True)\n(output_dir / 'figures').mkdir(parents=True, exist_ok=True)\n(output_dir / 'cost').mkdir(parents=True, exist_ok=True)\n\nreal_config = RealDatasetConfig(\n    mimic_root=Path(config.mimic_root),\n    sbdh_path=Path(config.sbdh_path),\n    category_filter=config.category_filter,\n    chunksize=config.chunksize,\n)\n\n# Load data (chunked)\ntry:\n    dataset = load_mimic_sbdh_dataset(real_config)\nexcept Exception as exc:\n    raise RuntimeError(\n        'Failed to load MIMIC/SBDH data. Ensure paths are correct and labels include SUBJECT_ID + HADM_ID (preferred) '\n        'or SUBJECT_ID + NOTE_ID.\n'\n        f'Error: {exc}'\n    )\n\nlabel_columns = [\n    col for col in dataset.columns\n    if col not in {'subject_id', 'hadm_id', 'note_id', 'text'}\n]\nif config.label_columns:\n    label_columns = config.label_columns\n\nprint('Loaded rows:', len(dataset))\nprint('Detected labels:', label_columns)\n\n# Dataset summary\nsummary = {\n    'n_notes': int(len(dataset)),\n    'n_subjects': int(dataset['subject_id'].nunique()),\n    'n_hadm': int(dataset['hadm_id'].nunique()) if 'hadm_id' in dataset.columns else None,\n    'label_prevalence': {},\n}\nfor label in label_columns:\n    summary['label_prevalence'][label] = float(dataset[label].mean())\n\nsummary_path = output_dir / 'dataset_summary.json'\nsummary_path.write_text(json.dumps(summary, indent=2))\nprint('Wrote summary to', summary_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## D. Task Operationalisation & Splits"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from sklearn.model_selection import GroupShuffleSplit\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n\nX = dataset['text'].fillna('')\ny = dataset[label_columns].astype(int)\nsubjects = dataset['subject_id']\n\n# Subject-level labels for leakage-free splits\nsubject_labels = dataset.groupby('subject_id')[label_columns].max().reset_index()\n\nif config.task_type == 'multilabel':\n    splitter = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=config.test_size, random_state=SEED)\n    subj_train_val_idx, subj_test_idx = next(splitter.split(subject_labels['subject_id'], subject_labels[label_columns]))\n    train_val_subjects = subject_labels.loc[subj_train_val_idx, 'subject_id']\n    test_subjects = subject_labels.loc[subj_test_idx, 'subject_id']\n\n    val_splitter = MultilabelStratifiedShuffleSplit(\n        n_splits=1,\n        test_size=config.val_size / (config.train_size + config.val_size),\n        random_state=SEED,\n    )\n    subj_train_idx, subj_val_idx = next(\n        val_splitter.split(train_val_subjects, subject_labels.loc[subj_train_val_idx, label_columns])\n    )\n    train_subjects = train_val_subjects.iloc[subj_train_idx]\n    val_subjects = train_val_subjects.iloc[subj_val_idx]\nelse:\n    gss = GroupShuffleSplit(n_splits=1, test_size=config.test_size, random_state=SEED)\n    subj_train_val_idx, subj_test_idx = next(\n        gss.split(subject_labels['subject_id'], subject_labels[label_columns].sum(axis=1), groups=subject_labels['subject_id'])\n    )\n    train_val_subjects = subject_labels.loc[subj_train_val_idx, 'subject_id']\n    test_subjects = subject_labels.loc[subj_test_idx, 'subject_id']\n\n    gss_val = GroupShuffleSplit(\n        n_splits=1,\n        test_size=config.val_size / (config.train_size + config.val_size),\n        random_state=SEED,\n    )\n    subj_train_idx, subj_val_idx = next(\n        gss_val.split(train_val_subjects, train_val_subjects, groups=train_val_subjects)\n    )\n    train_subjects = train_val_subjects.iloc[subj_train_idx]\n    val_subjects = train_val_subjects.iloc[subj_val_idx]\n\ntrain_df = dataset[dataset['subject_id'].isin(train_subjects)].copy()\nval_df = dataset[dataset['subject_id'].isin(val_subjects)].copy()\ntest_df = dataset[dataset['subject_id'].isin(test_subjects)].copy()\n\ntrain_df[['subject_id', *label_columns]].to_csv(output_dir / 'splits' / 'train.csv', index=False)\nval_df[['subject_id', *label_columns]].to_csv(output_dir / 'splits' / 'val.csv', index=False)\ntest_df[['subject_id', *label_columns]].to_csv(output_dir / 'splits' / 'test.csv', index=False)\n\nprint('Split sizes:', len(train_df), len(val_df), len(test_df))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## E. Baseline Models (Traditional ML)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score\nimport joblib\n\nif importlib.util.find_spec('xgboost'):\n    import xgboost as xgb\n    has_xgb = True\nelse:\n    has_xgb = False\n\nvectorizer = TfidfVectorizer(\n    max_features=40000,\n    ngram_range=(1, 2),\n    min_df=2,\n    lowercase=True,\n)\n\nX_train = vectorizer.fit_transform(train_df['text'])\nX_val = vectorizer.transform(val_df['text'])\nX_test = vectorizer.transform(test_df['text'])\n\nbaseline_results = []\n\nmodels = {\n    'LogReg': LogisticRegression(max_iter=200, class_weight='balanced', solver='saga', n_jobs=-1),\n    'RandomForest': RandomForestClassifier(n_estimators=200, class_weight='balanced', n_jobs=-1),\n}\nif has_xgb:\n    models['XGBoost'] = xgb.XGBClassifier(\n        n_estimators=200,\n        learning_rate=0.1,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        eval_metric='logloss',\n        tree_method='hist',\n    )\n\n\ndef count_params(clf):\n    total = 0\n    for est in getattr(clf, 'estimators_', []):\n        if hasattr(est, 'coef_'):\n            total += est.coef_.size\n            total += est.intercept_.size\n        elif hasattr(est, 'feature_importances_'):\n            total += est.feature_importances_.size\n    return total\n\nfor name, model in models.items():\n    clf = OneVsRestClassifier(model)\n    start = time.time()\n    clf.fit(X_train, train_df[label_columns])\n    train_time = time.time() - start\n\n    test_pred = clf.predict(X_test)\n    macro_f1 = f1_score(test_df[label_columns], test_pred, average='macro', zero_division=0)\n    micro_f1 = f1_score(test_df[label_columns], test_pred, average='micro', zero_division=0)\n\n    try:\n        val_scores = clf.predict_proba(X_val)\n        auc = roc_auc_score(val_df[label_columns], val_scores, average='macro')\n    except Exception:\n        auc = float('nan')\n\n    start_inf = time.perf_counter()\n    _ = clf.predict(X_test[:100])\n    inf_latency = time.perf_counter() - start_inf\n\n    baseline_results.append({\n        'model': name,\n        'macro_f1': macro_f1,\n        'micro_f1': micro_f1,\n        'auroc_macro': auc,\n        'train_time_s': train_time,\n        'inference_100_s': inf_latency,\n        'param_count': count_params(clf),\n    })\n\n    model_dir = output_dir / 'models'\n    model_dir.mkdir(exist_ok=True)\n    joblib.dump({'vectorizer': vectorizer, 'model': clf}, model_dir / f'{name}.joblib')\n\nprint(pd.DataFrame(baseline_results))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## F. Transformer Models (Clinical NLP)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from datasets import Dataset as HFDataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n\ntransformer_models = {\n    'BERT': 'bert-base-uncased',\n    'BioBERT': 'dmis-lab/biobert-base-cased-v1.2',\n    'ClinicalBERT': 'emilyalsentzer/Bio_ClinicalBERT',\n}\n\ntrain_texts = train_df['text'].tolist()\nval_texts = val_df['text'].tolist()\ntest_texts = test_df['text'].tolist()\n\ntrain_labels = train_df[label_columns].astype(int).values\nval_labels = val_df[label_columns].astype(int).values\ntest_labels = test_df[label_columns].astype(int).values\n\ntransformer_results = []\ntransformer_artifacts = {}\n\ndef tokenize_function(tokenizer, texts):\n    return tokenizer(texts, padding='max_length', truncation=True, max_length=config.max_length)\n\n\ndef optimize_thresholds(labels, probs):\n    thresholds = []\n    for i in range(labels.shape[1]):\n        best_thresh = 0.5\n        best_f1 = -1\n        for thresh in np.linspace(0.1, 0.9, 9):\n            preds = (probs[:, i] >= thresh).astype(int)\n            f1 = f1_score(labels[:, i], preds, zero_division=0)\n            if f1 > best_f1:\n                best_f1 = f1\n                best_thresh = thresh\n        thresholds.append(best_thresh)\n    return np.array(thresholds)\n\nfor name, checkpoint in transformer_models.items():\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n    train_enc = tokenize_function(tokenizer, train_texts)\n    val_enc = tokenize_function(tokenizer, val_texts)\n    test_enc = tokenize_function(tokenizer, test_texts)\n\n    train_ds = HFDataset.from_dict({**train_enc, 'labels': train_labels})\n    val_ds = HFDataset.from_dict({**val_enc, 'labels': val_labels})\n    test_ds = HFDataset.from_dict({**test_enc, 'labels': test_labels})\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        checkpoint,\n        num_labels=len(label_columns),\n        problem_type='multi_label_classification',\n    )\n\n    args = TrainingArguments(\n        output_dir=str(output_dir / 'models_transformers' / name),\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        num_train_epochs=1,\n        evaluation_strategy='epoch',\n        save_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='eval_loss',\n        logging_steps=50,\n        report_to='none',\n        seed=SEED,\n    )\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        probs = torch.sigmoid(torch.tensor(logits)).numpy()\n        preds = (probs >= 0.5).astype(int)\n        macro_f1 = f1_score(labels, preds, average='macro', zero_division=0)\n        micro_f1 = f1_score(labels, preds, average='micro', zero_division=0)\n        return {'macro_f1': macro_f1, 'micro_f1': micro_f1}\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        compute_metrics=compute_metrics,\n    )\n\n    start = time.time()\n    trainer.train()\n    train_time = time.time() - start\n\n    val_outputs = trainer.predict(val_ds)\n    val_probs = torch.sigmoid(torch.tensor(val_outputs.predictions)).numpy()\n    thresholds = optimize_thresholds(val_labels, val_probs)\n\n    test_outputs = trainer.predict(test_ds)\n    test_probs = torch.sigmoid(torch.tensor(test_outputs.predictions)).numpy()\n    preds = (test_probs >= thresholds).astype(int)\n\n    macro_f1 = f1_score(test_labels, preds, average='macro', zero_division=0)\n    micro_f1 = f1_score(test_labels, preds, average='micro', zero_division=0)\n    try:\n        auroc = roc_auc_score(test_labels, test_probs, average='macro')\n    except Exception:\n        auroc = float('nan')\n\n    start_inf = time.perf_counter()\n    _ = trainer.predict(test_ds.select(range(min(100, len(test_ds)))))\n    inf_latency = time.perf_counter() - start_inf\n\n    param_count = sum(p.numel() for p in model.parameters())\n\n    transformer_results.append({\n        'model': name,\n        'macro_f1': macro_f1,\n        'micro_f1': micro_f1,\n        'auroc_macro': auroc,\n        'train_time_s': train_time,\n        'inference_100_s': inf_latency,\n        'param_count': param_count,\n    })\n    transformer_artifacts[name] = {'model': model, 'tokenizer': tokenizer}\n\nprint(pd.DataFrame(transformer_results))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## G. Evaluation & Error Analysis"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\nimport textwrap\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmetrics_table = pd.DataFrame(baseline_results + transformer_results)\nmetrics_path = output_dir / 'metrics' / 'metrics_table.csv'\nmetrics_table.to_csv(metrics_path, index=False)\nprint('Wrote metrics table to', metrics_path)\n\n# Per-label metrics for last transformer model\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, preds, average=None, zero_division=0)\nper_label_df = pd.DataFrame({\n    'label': label_columns,\n    'precision': precision,\n    'recall': recall,\n    'f1': f1,\n})\n\nplt.figure(figsize=(10, 4))\nsns.barplot(data=per_label_df, x='label', y='f1')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nfig_path = output_dir / 'figures' / 'per_label_f1.png'\nplt.savefig(fig_path)\nplt.close()\n\n# Confusion matrix for most prevalent label\nprevalent_label = per_label_df.sort_values('f1', ascending=False)['label'].iloc[0]\nlabel_idx = label_columns.index(prevalent_label)\ncm = confusion_matrix(test_labels[:, label_idx], preds[:, label_idx])\nplt.figure(figsize=(4, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(f'Confusion Matrix: {prevalent_label}')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\ncm_path = output_dir / 'figures' / 'confusion_matrix.png'\nplt.tight_layout()\nplt.savefig(cm_path)\nplt.close()\n\nprint(classification_report(test_labels, preds, target_names=label_columns, zero_division=0))\n\n# Error analysis: clipped excerpts only\nNEGATION_PATTERN = re.compile(r\"\\b(no|denies|without|not|negative for)\\b\", re.IGNORECASE)\nTEMPORAL_PATTERN = re.compile(r\"\\b(history of|previous|prior|formerly|last year)\\b\", re.IGNORECASE)\nIMPLICIT_PATTERN = re.compile(r\"\\b(low income|food pantry|shelter|unstable housing)\\b\", re.IGNORECASE)\n\n\ndef categorize_error(text: str) -> str:\n    if NEGATION_PATTERN.search(text):\n        return 'negation'\n    if TEMPORAL_PATTERN.search(text):\n        return 'temporality'\n    if IMPLICIT_PATTERN.search(text):\n        return 'implicit'\n    return 'ambiguity'\n\nerrors = []\nfor gold, pred, text in zip(test_labels, preds, test_texts):\n    if not np.array_equal(gold, pred):\n        errors.append({\n            'text_excerpt': textwrap.shorten(text, width=280, placeholder='...'),\n            'error_type': categorize_error(text),\n            'gold': gold.tolist(),\n            'pred': pred.tolist(),\n        })\n    if len(errors) >= 5:\n        break\n\nerror_df = pd.DataFrame(errors)\nerror_df\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## H. Computational Cost Assessment"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import psutil\n\nif importlib.util.find_spec('fvcore'):\n    from fvcore.nn import FlopCountAnalysis\n    has_fvcore = True\nelse:\n    has_fvcore = False\n\nprocess = psutil.Process(os.getpid())\n\ncost_rows = []\nfor result in baseline_results + transformer_results:\n    model_name = result['model']\n    model_path = output_dir / 'models' / f'{model_name}.joblib'\n    if not model_path.exists():\n        model_path = output_dir / 'models_transformers' / model_name\n\n    size_mb = None\n    if model_path.exists():\n        if model_path.is_file():\n            size_mb = model_path.stat().st_size / 1e6\n        else:\n            size_mb = sum(p.stat().st_size for p in model_path.rglob('*') if p.is_file()) / 1e6\n\n    flop_estimate = None\n    if has_fvcore and model_name in transformer_artifacts:\n        model = transformer_artifacts[model_name]['model']\n        dummy = torch.randint(0, 100, (1, config.max_length))\n        dummy_mask = torch.ones_like(dummy)\n        try:\n            flop_estimate = FlopCountAnalysis(model, {'input_ids': dummy, 'attention_mask': dummy_mask}).total()\n        except Exception:\n            flop_estimate = None\n\n    cost_rows.append({\n        'model': model_name,\n        'train_time_s': result.get('train_time_s'),\n        'inference_100_s': result.get('inference_100_s'),\n        'param_count': result.get('param_count'),\n        'peak_rss_mb': process.memory_info().rss / 1e6,\n        'gpu_max_allocated_mb': torch.cuda.max_memory_allocated() / 1e6 if torch.cuda.is_available() else None,\n        'model_size_mb': size_mb,\n        'flops_estimate': flop_estimate,\n    })\n\ncost_table = pd.DataFrame(cost_rows)\ncost_path = output_dir / 'cost' / 'cost_table.csv'\ncost_table.to_csv(cost_path, index=False)\nprint('Wrote cost table to', cost_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## I. Reproducibility & Outputs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print('Outputs summary:')\nfor path in [\n    summary_path,\n    output_dir / 'splits' / 'train.csv',\n    output_dir / 'splits' / 'val.csv',\n    output_dir / 'splits' / 'test.csv',\n    metrics_path,\n    cost_path,\n    fig_path,\n    cm_path,\n]:\n    print('-', path.resolve())\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}